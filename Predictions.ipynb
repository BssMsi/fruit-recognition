{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for predicting label given model and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class Predict:\n",
    "    image_size = (100, 100)\n",
    "    def __init__(self, model, normalize=True):\n",
    "        '''\n",
    "        model:torchvision.models - pytorch model with loaded weights\n",
    "        normalize:model - if True, normalize with mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] for respective RGB channels\n",
    "        '''\n",
    "        self.model = model\n",
    "        if normalize:\n",
    "            normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            self.transformer = transforms.Compose([transforms.Resize(self.image_size), transforms.ToTensor(), normalizer])\n",
    "        else:\n",
    "            self.transformer = transforms.Compose([transforms.Resize(self.image_size), transforms.ToTensor()])            \n",
    "        self.normalize = normalize\n",
    "\n",
    "    def predict(self, source, source_type=\"infer\", file_type=\"infer\", batch_size=32, outFile=\"predictions.csv\"):\n",
    "        if source_type == \"infer\":\n",
    "            if \"http\" in source:\n",
    "                source_type = \"url\"\n",
    "                file_path = self.cache_file(source)\n",
    "            else:\n",
    "                source_type = \"local\"\n",
    "                file_path = source\n",
    "        elif source_type == \"local\":\n",
    "            file_path = source\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        dataloader = self.build_loader(file_path)\n",
    "        predictions = torch.empty(0, dtype=int)\n",
    "        self.model.eval()\n",
    "        for inputs in dataloader:\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = self.model.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                predictions = torch.cat((predictions, preds))\n",
    "        df = pd.DataFrame(predictions, columns=[\"class_idx\"])\n",
    "        labels_mapping = pd.read_csv(\"labels_mapping.csv\", header=None, index_col=0)\n",
    "        df[\"class_name\"] = df.class_idx.apply(lambda x: labels_mapping.loc[x])\n",
    "        if len(df) <= 10:\n",
    "            if input(\"Write predictions to file?(y/n)\").lower() == \"y\":\n",
    "                df.to_csv(outFile, header=True, index=False)\n",
    "            else:\n",
    "                print(df)\n",
    "        else:\n",
    "            df.to_csv(outFile, header=True, index=False)\n",
    "            print(f\"Written to '{outFile}'\")\n",
    "\n",
    "    def build_loader(self, path, batch_size=32):\n",
    "        if '.csv' in path.lower():\n",
    "            file_type = \"csv\"\n",
    "        elif \".jpg\" in path.lower() or \".jpeg\" in path.lower() or \".png\" in path.lower():\n",
    "            #return [transform_single(Image.open(path))]\n",
    "            # model expects 4 dimensions, hence unsqueeze to add dummy dimension\n",
    "            return [self.transformer(Image.open(path).convert('RGB')).unsqueeze(0)]\n",
    "        else:    # Assuming source to be a directory with all the images\n",
    "            from glob import glob\n",
    "            files = glob(f\"{path}/*\")\n",
    "            return datagen(files, batch_size)\n",
    "\n",
    "    def datagen(self, files, batch_size):\n",
    "        for i in range(0, len(files), batch_size):\n",
    "            yield torch.stack(tuple(map(lambda f: self.transformer(Image.open(f)), files)))    # Index exceeded is surprisingly handled automatically\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_single(img, input_size=100):\n",
    "        # Repeat mean and std to match image size\n",
    "        # Check if means matches their respective channels\n",
    "        m = np.repeat(np.repeat(np.array([0.485, 0.456, 0.406]).reshape(-1, 1), input_size, axis=1).reshape(3, input_size, 1), input_size, axis=2)\n",
    "        s = np.repeat(np.repeat(np.array([0.229, 0.224, 0.225]).reshape(-1, 1), input_size, axis=1).reshape(3, input_size, 1), input_size, axis=2)\n",
    "        # np.transpose since model expects (channel, height, width)\n",
    "        return torch.from_numpy(((np.transpose(img.resize((input_size, input_size)), (2, 0, 1)) - m) / s).reshape(1, 3, input_size, input_size)).type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def cache_file(url, cachedir=\"cache\"):\n",
    "        import requests\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()\n",
    "        # Save to local\n",
    "        local_file = f\"{cachedir}/{url.rsplit('/', 1)[-1]}\"\n",
    "        with open(local_file, 'wb') as fw:\n",
    "            fw.write(res.content)\n",
    "        return local_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 131\n",
    "\n",
    "model_path = \"weights/mobilenet\"\n",
    "weights = torch.load(model_path)\n",
    "model = models.mobilenet_v2(pretrained=False)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of using Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single image from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 100, 100])\n",
      "Write predictions to file?(y/n)n\n",
      "   class_idx class_name\n",
      "0         99  Pineapple\n"
     ]
    }
   ],
   "source": [
    "Predict(model).predict('images/Test/Pineapple/3_100.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single image from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write predictions to file?(y/n)n\n",
      "   class_idx        class_name\n",
      "0        117  Strawberry Wedge\n"
     ]
    }
   ],
   "source": [
    "url = \"https://5.imimg.com/data5/PW/ND/MY-46595757/fresh-pineapple-281kg-29-500x500.png\"\n",
    "Predict(model).predict(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Peach Flat\n",
      "Pear 2\n",
      "Peach Flat\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n",
      "Pear 2\n"
     ]
    }
   ],
   "source": [
    "# Live webcam capture\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)    # Set Width\n",
    "cap.set(4, 480)    # Set Height\n",
    "labels_mapping = pd.read_csv(\"labels_mapping.csv\", header=None, index_col=0)\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        #frame = torch.tensor(np.tile(frame, (1, 1, 1, 1)).transpose(0, 3, 1, 2))\n",
    "        frame = transform(frame)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            output = model.forward(frame)\n",
    "            _, pred = torch.max(output, 1)\n",
    "\n",
    "        print(labels_mapping.loc[int(pred), 1])\n",
    "        k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting video\n",
    "        if k == 27:\n",
    "            break\n",
    "except:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
